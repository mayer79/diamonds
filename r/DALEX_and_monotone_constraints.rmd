---
title: "DALEX - Descriptive mAchine Learning EXplanations"
output:
  html_document:
    df_print: paged
---

## Initializing stuff

```{r}
options(warn=-1)

library(ranger)
library(gbm)
library(DALEX) # For model interpretation
library(ceterisParibus)
library(tidyverse) # For data prep
library(caret) # For data split

rmse <- function(y, pred) {
  sqrt(sum((y - pred)^2))
}

head(diamonds) 

diamonds <- diamonds %>% 
  mutate_if(is.ordered, as.numeric) %>% 
  mutate(log_price = log(price),
         log_carat = log(carat)) 

x <- c("log_carat", "cut", "color", "clarity", "depth", "table")
y <- "log_price"

# Train/test split
set.seed(3928272)
ind <- caret::createDataPartition(diamonds[[y]], p = 0.85, list = FALSE) %>% c

trainDF <- diamonds[ ind, c(y, x)]
validDF <- diamonds[-ind, c(y, x)]
```


## Fit the models: Linear model, boosted trees, random forest
```{r}
form <- reformulate(x, y)

summary(lm_model <- lm(form, data = trainDF))

gbm_model <- gbm(form, data = trainDF, 
                 n.trees = 200, 
                 interaction.depth = 3, 
                 shrinkage = 0.05, 
                 var.monotone = c(1, 1, 0, 0, 0, 0))

rf_model <- ranger(form, data = trainDF, num.trees = 200)
```

## Initializing the "explainer"
```{r}
explainer_lm <- explain(lm_model, data = validDF[x], y = validDF[[y]], label = "lm")
explainer_gbm <- explain(gbm_model, data = validDF[x], y = validDF[[y]], label = "gbm",
                         predict_function = function(model, x) predict(model, x, n.trees = 200))
explainer_rf <- explain(rf_model, data = validDF[x], y = validDF[[y]], label = "rf")

explainers <- list(explainer_lm, explainer_gbm, explainer_rf)
```

## Model performance on validation data
```{r}
mp <- lapply(explainers, model_performance)
do.call(plot, mp)
```

## Variable importance on permuting validation data
```{r}
vd <- lapply(explainers, variable_importance)
do.call(plot, vd)

```

## Variable importance ("prediction breakdown") for one single observation
```{r, fig.height=10}
sp <- lapply(explainers, single_prediction, observation = validDF[1, ])
do.call(plot, sp) # almost the full effect comes from carat
```


## Relationship between log_carat and log_price: ceteris paribus profiles across models
```{r}
picks <- 1
cp <- lapply(explainers, 
             ceteris_paribus, 
             observations = validDF[picks, x] %>% as.data.frame)

do.call(plot, c(cp, color = "_label_", selected_variables = "log_carat"))
```

## Relationship between log_carat and log_price: multiple ceteris paribus profiles for the random forest
```{r}
picks <- 1:20
cp <- ceteris_paribus(explainer_rf, observations = validDF[picks, x] %>% as.data.frame)
plot(cp, selected_variables = "log_carat")
```

## Relationship between log_carat and log_price: average of above profiles = "partial dependence plots"
```{r}
plot(cp, selected_variables = "log_carat", aggregate_profiles = mean)
```

## Variant: Look at profiles of close neighbours
```{r}
neigh <- select_neighbours(validDF, validDF[1, ])
cp <- ceteris_paribus(explainer_rf, observations = neigh %>% as.data.frame)
plot(cp, selected_variables = "log_carat")
```